{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Caras lab ephys analysis pipeline\n",
    "This pipeline is intended to be run after extracting behavioral timestamps and neuron spike times with our [MatLab pipeline](https://github.com/caraslab/caraslab-spikesortingKS2)\n",
    "\n",
    "Files need to be organized in a specific folder structure or file paths need to be changed\n",
    "\n",
    "File structure can be found in the Sample dataset folder"
   ],
   "id": "344dc9b19a3e6c21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Imports\n",
    "Specific imports can be found within each function"
   ],
   "id": "8adc8165366abc8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T19:39:33.814293Z",
     "start_time": "2024-12-09T19:38:39.791619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from os import remove, makedirs\n",
    "import warnings\n",
    "from os.path import sep\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from glob import glob\n",
    "\n",
    "from helpers.run_ephys_pipeline import run_pipeline\n",
    "from helpers.compile_fr_result_csv import compile_fr_result_csv\n",
    "from matplotlib.pyplot import rcParams\n",
    "from helpers.get_JSON_data import get_JSON_data\n",
    "from helpers.PSTH_plotter_fromJSON import run_PSTH_pipeline\n",
    "from helpers.auROC_heatmap_plotter import run_auROC_heatmap_pipeline\n",
    "from helpers.auROC_tslearnClustering_mp import run_ts_clustering\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Some plotting parameters\n",
    "label_font_size = 11\n",
    "tick_label_size = 7\n",
    "legend_font_size = 6\n",
    "line_thickness = 1\n",
    "\n",
    "rcParams['figure.dpi'] = 600\n",
    "rcParams['pdf.fonttype'] = 42\n",
    "rcParams['ps.fonttype'] = 42\n",
    "rcParams['font.family'] = 'Arial'\n",
    "rcParams['font.weight'] = 'regular'\n",
    "rcParams['axes.labelweight'] = 'regular'\n",
    "\n",
    "rcParams['font.size'] = label_font_size\n",
    "rcParams['axes.labelsize'] = label_font_size\n",
    "rcParams['axes.titlesize'] = label_font_size\n",
    "rcParams['axes.linewidth'] = line_thickness\n",
    "rcParams['legend.fontsize'] = legend_font_size\n",
    "rcParams['xtick.labelsize'] = tick_label_size\n",
    "rcParams['ytick.labelsize'] = tick_label_size\n",
    "rcParams['errorbar.capsize'] = label_font_size\n",
    "rcParams['lines.markersize'] = line_thickness\n",
    "rcParams['lines.linewidth'] = line_thickness"
   ],
   "id": "30ef205fa68127e6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\My Drive\\Documents\\PycharmProjects\\Caraslab_EPhys_analysis_pipeline\\venv\\Lib\\site-packages\\tslearn\\bases\\bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Set global paths and variables\n",
    "\n",
    "FILE NAMING REQUIREMENTS\n",
    "\n",
    "This pipeline matches files using filenames.\n",
    "\n",
    "If you make changes to the filenaming structures you will have to edit the filename matching code\n",
    "\n",
    "Here are the defaults that come out of the MatLab processing pipeline:\n",
    "- Synapse:\n",
    "    - Behavior file: SUBJ-ID-154_MML-Aversive-AM-210501-112033_trialInfo.csv\n",
    "    - Spike time file: SUBJ-ID-154_210501_concat_cluster2627.txt\n",
    "- Intan:\n",
    "    - Behavior file: SUBJ-ID-231_2021-07-17_15-19-28_Active_trialInfo.csv\n",
    "    - Spike time file: SUBJ-ID-231_210717_concat_cluster651.txt\n",
    "\n",
    "If you need to alter the filename matching structure, edit this function: helpers.preprocess_files.find_spoutfile_and_breakpoint"
   ],
   "id": "205fcc63f736d166"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:53:39.011990Z",
     "start_time": "2024-12-09T20:53:36.202406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_PATH = '.' + sep + 'Sample_data'\n",
    "\n",
    "SETTINGS_DICT = {\n",
    "    'EXPERIMENT_TAG': 'OFCPL',  # Appends to start of summary files\n",
    "    'SPIKES_PATH': DATA_PATH + sep + 'Spike times',\n",
    "    'KEYS_PATH': DATA_PATH + sep + 'Key files',\n",
    "    'OUTPUT_PATH': DATA_PATH + sep + 'Output',\n",
    "    \n",
    "    # If your data was concatenated before Kilosort\n",
    "    'BREAKPOINT_PATH': DATA_PATH + sep + 'Breakpoints',\n",
    "    \n",
    "    # in seconds; for firing rate calculation to non-AM trials\n",
    "    'NONAM_DURATION_FOR_FR': 1,  \n",
    "    \n",
    "    # in seconds; for firing rate calculation to AM trials; ignore shock artifact starting early\n",
    "    'TRIAL_DURATION_FOR_FR': {'Hit': 0.95, 'FA': 0.95, 'Miss': 0.95},\n",
    "    'AFTERTRIAL_FR_START': {'Hit': 1, 'FA': 1, 'Miss': 1.3},  # Shock artifact is ~0.3s long\n",
    "    'AFTERTRIAL_FR_END': {'Hit': 1.9, 'FA': 1.9, 'Miss': 2.25},  # 0.95 duration to keep window consistent with trial\n",
    "    \n",
    "    # Unused for now but keep this period in mind when analyzing shocked misses\n",
    "    'SHOCK_ARTIFACT': [0.95, 1.3],\n",
    "    \n",
    "    # These are for raw data extraction (no FR calculations)\n",
    "    'PRETRIAL_DURATION_FOR_SPIKETIMES': 2,  # in seconds; for grabbing spiketimes around AM trials\n",
    "    'POSTTRIAL_DURATION_FOR_SPIKETIMES': 5,  # in seconds; for grabbing spiketimes around AM trials\n",
    "    \n",
    "    'MULTIPROCESS': True,  # Turn off multiprocessing for easier debugging\n",
    "    \n",
    "    # For multiprocessing. Defaults to 4/5s of the number of cores\n",
    "    'NUMBER_OF_CORES': 4 * cpu_count() // 5,\n",
    "\n",
    "    # Only run these cells/subjects/sessions or None to run all\n",
    "    'SESSIONS_TO_RUN': None,  # You can specify parts of the file name too\n",
    "    'SESSIONS_TO_EXCLUDE': None,\n",
    "    \n",
    "    'OVERWRITE_PREVIOUS_CSV': True,  # False: appends to existing firing rate CSV file\n",
    "    \n",
    "    # Set up your recording platforms here\n",
    "    'RECORDING_TYPE_DICT': {\n",
    "        'SUBJ-ID-197': 'synapse',\n",
    "        'SUBJ-ID-151': 'synapse',\n",
    "        'SUBJ-ID-154': 'synapse',\n",
    "        'SUBJ-ID-231': 'intan',\n",
    "        # 'SUBJ-ID-232': 'intan',\n",
    "        # 'SUBJ-ID-270': 'intan',\n",
    "        'SUBJ-ID-389': 'intan',\n",
    "        'SUBJ-ID-390': 'intan'\n",
    "    },\n",
    "\n",
    "    # Only needed for some older recordings processed by the MatLab pipeline. Newer files contain the sampling rates in them\n",
    "    'SAMPLING_RATE_DICT': {\n",
    "        'synapse': 24414.0625,\n",
    "        'intan': 30000\n",
    "    },\n",
    "    \n",
    "    # PSTH generation settings\n",
    "    'PSTH_BIN_SIZE': 10,\n",
    "    'PSTH_PRE_STIMULUS_DURATION': 2,\n",
    "    'PSTH_POST_STIMULUS_DURATION': 4,\n",
    "    'PSTH_FIXED_YLIM': 60,\n",
    "    'PSTH_RASTER_YLIM': 30.5,\n",
    "    'PSTH_ALIGN_TO_RESPONSE': False,\n",
    "    'PSTH_TRIALTYPES': [\n",
    "        'Hit (shock)',  # Trials above threshold\n",
    "        # 'Hit (no shock)', # Trials below threshold\n",
    "        'False alarm',\n",
    "        'Miss (shock)',  # Trials above threshold\n",
    "        # 'Miss (no shock)', # Trials below threshold\n",
    "        'Passive',\n",
    "    ],\n",
    "    \n",
    "    #########################################\n",
    "    # AuROC heatmap generation settings\n",
    "    #########################################\n",
    "    \n",
    "    # This grouping file can contain any number of characteristics, such as cell type, response characteristics, clustering IDs etc. Has to be generated elsewhere; Otherwise set to None\n",
    "    'AUROC_GROUPING_FILE': None, # DATA_PATH + sep + 'allUnits_list.csv',  # Or None\n",
    "    'AUROC_GROUPING_VARIABLE': 'ActiveBaseline_modulation_direction',  # Or None\n",
    "    'AUROC_UNIQUE_GROUPS': ['decrease', 'increase', 'none'],\n",
    "    'AUROC_GROUP_COLORS': ['#E49E50', '#5AB4E5', '#939598'],\n",
    "    \n",
    "    'AUROC_TRIALTYPES': {\n",
    "        # Use these if you would like to sort based on trial (AM)-aligned auROC\n",
    "        'PassivePre': [0, 1.5],\n",
    "        'Hit_shockOn_auroc': [0, 1.5],\n",
    "        # 'Hit_shockOff_auroc': [0, 1.5],\n",
    "        'FA_auroc': [0, 1.5],\n",
    "        'Miss_shockOn_auroc': [1.4, 2.9],\n",
    "        # 'Miss_shockOff_auroc': [1.4, 2.9]\n",
    "        'PassivePost': [0, 1.5],\n",
    "        \n",
    "        # Use these if you would like to sort based on response (spout offset)-aligned auROC\n",
    "        # 'SpoutOff_hits_auroc': [-0.5, 0],\n",
    "        # 'SpoutOff_FAs_auroc': [-0.5, 0],\n",
    "        # 'SpoutOff_misses_auroc': [-0.5, 0]\n",
    "    },\n",
    "    \n",
    "    'SORT_BY_WHICH_TRIALTYPE': 'Hit_shockOn_auroc',\n",
    "    \n",
    "    'AUROC_BIN_SIZE': 0.1,\n",
    "    'AUROC_PRE_STIMULUS_DURATION': 2,\n",
    "    \n",
    "    'AUROC_POST_STIMULUS_DURATION': 4,\n",
    "    \n",
    "    #########################################\n",
    "    # Time series clustering parameters\n",
    "    #########################################\n",
    "    'MAXCLUSTERS': 10,  # maximum number of clusters to be evaluated (10)\n",
    "    'BOOT_N': 50,  # number of iterations (maximum of 50 recommended due to computing time for DTW) (50)\n",
    "    'SK_FACTOR': 2,  # error multiplication factor for choosing best number of clusters (2)\n",
    "    'USE_GPU': False,  # If true, overrides the global multiprocessing flag; Not implemented yet\n",
    "    'TS_TRIALTYPES': {\n",
    "        # Use these if you would like to cluster based on trial (AM)-aligned auROC\n",
    "        # 'PassivePre': [0, 1.5],\n",
    "        # 'Hit_shockOn_auroc': [0, 1.5],\n",
    "        # 'Hit_shockOff_auroc': [0, 1.5],\n",
    "        # 'FA_auroc': [0, 1.5],\n",
    "        # 'Miss_shockOn_auroc': [1.4, 2.9],\n",
    "        # 'Miss_shockOff_auroc': [1.4, 2.9]\n",
    "        # 'PassivePost': [0, 1.5],\n",
    "        \n",
    "        # Use these if you would like to cluster based on response (spout offset)-aligned auROC\n",
    "        'SpoutOff_hits_auroc': [0, 1.5],\n",
    "        'SpoutOff_FAs_auroc': [0, 1.5],\n",
    "        'SpoutOff_misses_auroc': [0.3, 1.8],\n",
    "    },\n",
    "    \n",
    "    #########################################\n",
    "    # Switchboard \n",
    "    # Below is a switchboard of functions you desire to run from the pipeline\n",
    "    # If you change your mind later, you can just run the ones you want and the code will add it to existing JSON files\n",
    "    #########################################\n",
    "    'PIPELINE_SWITCHBOARD': {\n",
    "        # Trial-by-trial firing rates\n",
    "        'firing_rate_to_trials': True,  \n",
    "        \n",
    "        # auROC analyses need by definition to combine trials, so each of these pool a specific type of trial and align to specific events\n",
    "        # Trials aligned by AM onset\n",
    "        'auROC_hit': True,\n",
    "        'auROC_FA': True,\n",
    "        'auROC_miss': True,\n",
    "        'auROC_hitByShock': True,  # Separate trials by whether a shock was going to be delivered\n",
    "        'auROC_missByShock': True, # Separate trials by whether a shock was delivered\n",
    "        'auROC_AMTrial': True,  # Agnostic of whether miss or hit\n",
    "        'auROC_AMdepthByAMdepth': False, # Agnostic of whether miss or hit, but separated by AM depth\n",
    "        \n",
    "        # Trials aligned by spoutOff events\n",
    "        'auROC_spoutOffHit': True,\n",
    "        'auROC_spoutOffFA': True,\n",
    "        'auROC_spoutOffMiss': True,\n",
    "        'auROC_spoutOffHitByShock': True,  # Separate trials by whether a shock was going to be delivered\n",
    "        'auROC_spoutOffMissByShock': True, # Separate trials by whether a shock was delivered\n",
    "        \n",
    "        # PSTH plotting\n",
    "        'plot_trialType_PSTH': True,\n",
    "        'plot_AMDepth_PSTH': True\n",
    "    }\n",
    "    #########################################\n",
    "}\n"
   ],
   "id": "5f703569dd7af350",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initial file matching then run the pipeline\n",
    "Uses multiprocessing to process many units at once"
   ],
   "id": "731e823a8fb4dcf8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:07:56.366285Z",
     "start_time": "2024-12-09T20:05:06.094116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "makedirs(SETTINGS_DICT['OUTPUT_PATH'] + sep + 'JSON files', exist_ok=True)\n",
    "\n",
    "# Load existing JSONs; will be empty if this is the first time running\n",
    "json_filenames = glob(SETTINGS_DICT['OUTPUT_PATH'] + sep + 'JSON files' + sep + '*json')\n",
    "\n",
    "# Clear older temp files if they exist\n",
    "process_tempfiles = glob(SETTINGS_DICT['OUTPUT_PATH'] + sep + '*_tempfile_*.csv')\n",
    "[remove(f) for f in process_tempfiles]\n",
    "\n",
    "# Generate a list of inputs to be passed to each worker\n",
    "input_lists = list()\n",
    "memory_paths = glob(SETTINGS_DICT['SPIKES_PATH'] + sep + '*cluster*.txt')\n",
    "\n",
    "for dummy_idx, memory_path in enumerate(memory_paths):\n",
    "    if SETTINGS_DICT['SESSIONS_TO_RUN'] is not None:\n",
    "        if any([chosen for chosen in SETTINGS_DICT['SESSIONS_TO_RUN'] if chosen in memory_path]):\n",
    "            pass\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    if SETTINGS_DICT['SESSIONS_TO_EXCLUDE'] is not None:\n",
    "        if any([chosen for chosen in SETTINGS_DICT['SESSIONS_TO_EXCLUDE'] if chosen in memory_path]):\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    if SETTINGS_DICT['MULTIPROCESS']:\n",
    "        input_lists.append((memory_path, json_filenames, SETTINGS_DICT))\n",
    "    else:\n",
    "        run_pipeline((memory_path, json_filenames, SETTINGS_DICT))\n",
    "\n",
    "if SETTINGS_DICT['MULTIPROCESS']:\n",
    "    pool = Pool(SETTINGS_DICT['NUMBER_OF_CORES'])\n",
    "\n",
    "    # # Feed each worker with all memory paths from one unit\n",
    "    pool_map_result = pool.map(run_pipeline, input_lists)\n",
    "\n",
    "    pool.close()\n",
    "\n",
    "    pool.join()\n",
    "    \n",
    "    compile_fr_result_csv(SETTINGS_DICT['EXPERIMENT_TAG'], SETTINGS_DICT['OUTPUT_PATH'], SETTINGS_DICT['OVERWRITE_PREVIOUS_CSV'])"
   ],
   "id": "7bed86f7c4752178",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Now we can use the JSONs exported from the code above for a faster exploration of the data",
   "id": "deea365829bb96c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:21:57.963036Z",
     "start_time": "2024-12-09T20:21:24.192306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sessions you want to run; edit the specifics in helpers.get_JSON_data if you want to change\n",
    "session_tags = ['pre', 'active', 'post', 'post1h']\n",
    "\n",
    "# Load existing JSONs\n",
    "json_filenames = glob(SETTINGS_DICT['OUTPUT_PATH'] + sep + 'JSON files' + sep + '*json')\n",
    "\n",
    "# Retrieve data in JSON\n",
    "data_dict = dict()\n",
    "print(\"Loading data in JSONs...\")\n",
    "for session_type in session_tags:\n",
    "    unit_list, data_list = (\n",
    "        get_JSON_data(json_filenames, session_type, \n",
    "                        sessions_to_run=SETTINGS_DICT['SESSIONS_TO_RUN'], \n",
    "                        sessions_to_exclude=SETTINGS_DICT['SESSIONS_TO_EXCLUDE'])\n",
    "    )\n",
    "    \n",
    "    for unit_idx, unit in enumerate(unit_list):\n",
    "        if unit not in data_dict.keys():\n",
    "            data_dict.update({unit: {}})\n",
    "        data_dict[unit].update({session_type: data_list[unit_idx]})"
   ],
   "id": "5fc8863493437641",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data in JSONs...\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PSTH plotting\n",
    "Unsophisticated PSTH plotting engine. Need to make tweaks if you want something fancier\n",
    "\n",
    "Uses multiprocessing to process many units at once"
   ],
   "id": "3f0dec45a485d216"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:28:05.266141Z",
     "start_time": "2024-12-09T20:23:45.289100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if SETTINGS_DICT['MULTIPROCESS']:\n",
    "    pool = Pool(SETTINGS_DICT['NUMBER_OF_CORES'])\n",
    "\n",
    "    # Feed each worker one unit_name\n",
    "    input_list = [(unit_name, data_dict[unit_name], SETTINGS_DICT) for unit_name in data_dict.keys()]\n",
    "    pool_map_result = pool.map(run_PSTH_pipeline, input_list)\n",
    "\n",
    "    pool.close()\n",
    "\n",
    "    pool.join()\n",
    "else:\n",
    "    [run_PSTH_pipeline((unit_name, data_dict[unit_name], SETTINGS_DICT)) for unit_name in data_dict.keys()]"
   ],
   "id": "8042d360245ce834",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] The system cannot find the path specified: '.\\\\Sample_data\\\\Output\\\\PSTHs\\\\SUBJ-ID-154\\\\TrialType'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRemoteTraceback\u001B[0m                           Traceback (most recent call last)",
      "\u001B[1;31mRemoteTraceback\u001B[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\caraslab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\caraslab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py\", line 48, in mapstar\n    return list(map(*args))\n           ^^^^^^^^^^^^^^^^\n  File \"G:\\My Drive\\Documents\\PycharmProjects\\Caraslab_EPhys_analysis_pipeline\\helpers\\PSTH_plotter_fromJSON.py\", line 341, in run_PSTH_pipeline\n    makedirs(output_subfolder, exist_ok=True)\n  File \"<frozen os>\", line 225, in makedirs\nFileNotFoundError: [WinError 3] The system cannot find the path specified: '.\\\\Sample_data\\\\Output\\\\PSTHs\\\\SUBJ-ID-154\\\\TrialType'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Feed each worker one unit_name\u001B[39;00m\n\u001B[0;32m      5\u001B[0m input_list \u001B[38;5;241m=\u001B[39m [(unit_name, data_dict[unit_name], SETTINGS_DICT) \u001B[38;5;28;01mfor\u001B[39;00m unit_name \u001B[38;5;129;01min\u001B[39;00m data_dict\u001B[38;5;241m.\u001B[39mkeys()]\n\u001B[1;32m----> 6\u001B[0m pool_map_result \u001B[38;5;241m=\u001B[39m \u001B[43mpool\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrun_PSTH_pipeline\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_list\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m pool\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m     10\u001B[0m pool\u001B[38;5;241m.\u001B[39mjoin()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py:367\u001B[0m, in \u001B[0;36mPool.map\u001B[1;34m(self, func, iterable, chunksize)\u001B[0m\n\u001B[0;32m    362\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmap\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, iterable, chunksize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    363\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m    364\u001B[0m \u001B[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001B[39;00m\n\u001B[0;32m    365\u001B[0m \u001B[38;5;124;03m    in a list that is returned.\u001B[39;00m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;124;03m    '''\u001B[39;00m\n\u001B[1;32m--> 367\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapstar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py:774\u001B[0m, in \u001B[0;36mApplyResult.get\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    772\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value\n\u001B[0;32m    773\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 774\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] The system cannot find the path specified: '.\\\\Sample_data\\\\Output\\\\PSTHs\\\\SUBJ-ID-154\\\\TrialType'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AuROC heatmaps\n",
    "Quick summary of what auROC profiles look like across the entire population."
   ],
   "id": "3801fee70507f18"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:52:27.490300Z",
     "start_time": "2024-12-09T20:52:23.540450Z"
    }
   },
   "cell_type": "code",
   "source": "run_auROC_heatmap_pipeline(data_dict, SETTINGS_DICT)",
   "id": "6d717d51aa97e18c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Time series clustering\n",
    "This function calculates the optimal number of clusters from time series\n",
    "\n",
    "It is optimal for time series because it uses dynamic time warping (DTW) as a distance metric\n",
    "\n",
    "The optimal number of clusters is derived from an implementation of the gap-statistic by Tibshirani et al., 2001\n"
   ],
   "id": "5b877529678ec4ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T21:33:46.503827Z",
     "start_time": "2024-12-09T21:21:46.899846Z"
    }
   },
   "cell_type": "code",
   "source": "run_ts_clustering(data_dict, SETTINGS_DICT)",
   "id": "8bd77d1d93171632",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running gap-stat clustering for metric SpoutOff_hits_auroc with 10 maxclusters and 50 simulations\n",
      "Loading data in JSONs...\n",
      "OptimalK start...\n",
      "Clustering data using 1 cluster(s)...\n",
      "Processing time: 0 min, 0.359 sec\n",
      "Clustering data using 2 cluster(s)...\n",
      "Processing time: 0 min, 1.046 sec\n",
      "Clustering data using 3 cluster(s)...\n",
      "Processing time: 0 min, 1.558 sec\n",
      "Clustering data using 4 cluster(s)...\n",
      "Processing time: 0 min, 1.247 sec\n",
      "Clustering data using 5 cluster(s)...\n",
      "Processing time: 0 min, 1.434 sec\n",
      "Clustering data using 6 cluster(s)...\n",
      "Processing time: 0 min, 1.187 sec\n",
      "Clustering data using 7 cluster(s)...\n",
      "Processing time: 0 min, 1.318 sec\n",
      "Clustering data using 8 cluster(s)...\n",
      "Processing time: 0 min, 1.186 sec\n",
      "Clustering data using 9 cluster(s)...\n",
      "Processing time: 0 min, 1.304 sec\n",
      "Clustering data using 10 cluster(s)...\n",
      "Processing time: 0 min, 1.654 sec\n",
      "Initializing 12 process(es) for clustering random distributions in 50 simulations x 10 clusters = 500 iterations\n",
      "Processing time: 3 min, 45.221 sec\n",
      "Compiling data and computing gap-statistic...\n",
      "Processing time: 0 min, 0.354 sec\n",
      "Gap-statistic found 4 clusters in data\n",
      "Total runtime: 4 min, 4.549 sec\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Running gap-stat clustering for metric SpoutOff_FAs_auroc with 10 maxclusters and 50 simulations\n",
      "Loading data in JSONs...\n",
      "OptimalK start...\n",
      "Clustering data using 1 cluster(s)...\n",
      "Processing time: 0 min, 0.563 sec\n",
      "Clustering data using 2 cluster(s)...\n",
      "Processing time: 0 min, 0.769 sec\n",
      "Clustering data using 3 cluster(s)...\n",
      "Processing time: 0 min, 1.213 sec\n",
      "Clustering data using 4 cluster(s)...\n",
      "Processing time: 0 min, 1.229 sec\n",
      "Clustering data using 5 cluster(s)...\n",
      "Processing time: 0 min, 1.084 sec\n",
      "Clustering data using 6 cluster(s)...\n",
      "Processing time: 0 min, 0.894 sec\n",
      "Clustering data using 7 cluster(s)...\n",
      "Processing time: 0 min, 1.542 sec\n",
      "Clustering data using 8 cluster(s)...\n",
      "Processing time: 0 min, 1.266 sec\n",
      "Clustering data using 9 cluster(s)...\n",
      "Processing time: 0 min, 1.526 sec\n",
      "Clustering data using 10 cluster(s)...\n",
      "Processing time: 0 min, 2.276 sec\n",
      "Initializing 12 process(es) for clustering random distributions in 50 simulations x 10 clusters = 500 iterations\n",
      "Processing time: 3 min, 30.706 sec\n",
      "Compiling data and computing gap-statistic...\n",
      "Processing time: 0 min, 0.378 sec\n",
      "Gap-statistic found 3 clusters in data\n",
      "Total runtime: 3 min, 49.422 sec\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Running gap-stat clustering for metric SpoutOff_misses_auroc with 10 maxclusters and 50 simulations\n",
      "Loading data in JSONs...\n",
      "OptimalK start...\n",
      "Clustering data using 1 cluster(s)...\n",
      "Processing time: 0 min, 0.586 sec\n",
      "Clustering data using 2 cluster(s)...\n",
      "Processing time: 0 min, 1.328 sec\n",
      "Clustering data using 3 cluster(s)...\n",
      "Processing time: 0 min, 0.968 sec\n",
      "Clustering data using 4 cluster(s)...\n",
      "Processing time: 0 min, 0.903 sec\n",
      "Clustering data using 5 cluster(s)...\n",
      "Processing time: 0 min, 0.977 sec\n",
      "Clustering data using 6 cluster(s)...\n",
      "Processing time: 0 min, 1.393 sec\n",
      "Clustering data using 7 cluster(s)...\n",
      "Processing time: 0 min, 1.091 sec\n",
      "Clustering data using 8 cluster(s)...\n",
      "Processing time: 0 min, 1.589 sec\n",
      "Clustering data using 9 cluster(s)...\n",
      "Processing time: 0 min, 1.817 sec\n",
      "Clustering data using 10 cluster(s)...\n",
      "Processing time: 0 min, 2.780 sec\n",
      "Initializing 12 process(es) for clustering random distributions in 50 simulations x 10 clusters = 500 iterations\n",
      "Processing time: 3 min, 43.177 sec\n",
      "Compiling data and computing gap-statistic...\n",
      "Processing time: 0 min, 0.285 sec\n",
      "Gap-statistic found 2 clusters in data\n",
      "Total runtime: 4 min, 2.717 sec\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3000x3000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3000x3000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3000x3000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
