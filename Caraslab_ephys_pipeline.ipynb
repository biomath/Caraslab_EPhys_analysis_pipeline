{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Caras lab ephys analysis pipeline\n",
    "This pipeline is intended to be run after extracting behavioral timestamps and neuron spike times with our [MatLab pipeline](https://github.com/caraslab/caraslab-spikesortingKS2)\n",
    "\n",
    "Files need to be organized in a specific folder structure or file paths need to be changed\n",
    "\n",
    "File structure can be found in the Sample dataset folder"
   ],
   "id": "344dc9b19a3e6c21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Imports\n",
    "Specific imports can be found within each function"
   ],
   "id": "8adc8165366abc8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from os import remove, makedirs\n",
    "import warnings\n",
    "from os.path import sep\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from glob import glob\n",
    "\n",
    "from helpers.run_ephys_pipeline import run_pipeline\n",
    "from helpers.compile_fr_result_csv import compile_fr_result_csv\n",
    "from matplotlib.pyplot import rcParams\n",
    "from helpers.get_JSON_data import get_JSON_data\n",
    "from helpers.PSTH_plotter_fromJSON import run_PSTH_pipeline\n",
    "from helpers.auROC_heatmap_plotter import run_auROC_heatmap_pipeline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Some plotting parameters\n",
    "label_font_size = 11\n",
    "tick_label_size = 7\n",
    "legend_font_size = 6\n",
    "line_thickness = 1\n",
    "\n",
    "rcParams['figure.dpi'] = 600\n",
    "rcParams['pdf.fonttype'] = 42\n",
    "rcParams['ps.fonttype'] = 42\n",
    "rcParams['font.family'] = 'Arial'\n",
    "rcParams['font.weight'] = 'regular'\n",
    "rcParams['axes.labelweight'] = 'regular'\n",
    "\n",
    "rcParams['font.size'] = label_font_size\n",
    "rcParams['axes.labelsize'] = label_font_size\n",
    "rcParams['axes.titlesize'] = label_font_size\n",
    "rcParams['axes.linewidth'] = line_thickness\n",
    "rcParams['legend.fontsize'] = legend_font_size\n",
    "rcParams['xtick.labelsize'] = tick_label_size\n",
    "rcParams['ytick.labelsize'] = tick_label_size\n",
    "rcParams['errorbar.capsize'] = label_font_size\n",
    "rcParams['lines.markersize'] = line_thickness\n",
    "rcParams['lines.linewidth'] = line_thickness"
   ],
   "id": "30ef205fa68127e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Set global paths and variables\n",
    "\n",
    "FILE NAMING REQUIREMENTS\n",
    "\n",
    "This pipeline matches files using filenames.\n",
    "\n",
    "If you make changes to the filenaming structures you will have to edit the filename matching code\n",
    "\n",
    "Here are the defaults that come out of the MatLab processing pipeline:\n",
    "- Synapse:\n",
    "    - Behavior file: SUBJ-ID-154_MML-Aversive-AM-210501-112033_trialInfo.csv\n",
    "    - Spike time file: SUBJ-ID-154_210501_concat_cluster2627.txt\n",
    "- Intan:\n",
    "    - Behavior file: SUBJ-ID-231_2021-07-17_15-19-28_Active_trialInfo.csv\n",
    "    - Spike time file: SUBJ-ID-231_210717_concat_cluster651.txt\n",
    "\n",
    "If you need to alter the filename matching structure, edit this function: helpers.preprocess_files.find_spoutfile_and_breakpoint"
   ],
   "id": "205fcc63f736d166"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DATA_PATH = '.' + sep + 'Sample_data'\n",
    "\n",
    "SETTINGS_DICT = {\n",
    "    'EXPERIMENT_TAG': 'OFCPL',  # Appends to start of summary files\n",
    "    'SPIKES_PATH': DATA_PATH + sep + 'Spike times',\n",
    "    'KEYS_PATH': DATA_PATH + sep + 'Key files',\n",
    "    'OUTPUT_PATH': DATA_PATH,\n",
    "    \n",
    "    # If your data was concatenated before Kilosort\n",
    "    'BREAKPOINT_PATH': DATA_PATH + sep + 'Breakpoints',\n",
    "    \n",
    "    # in seconds; for firing rate calculation to non-AM trials\n",
    "    'NONAM_DURATION_FOR_FR': 1,  \n",
    "    \n",
    "    # in seconds; for firing rate calculation to AM trials; ignore shock artifact starting early\n",
    "    'TRIAL_DURATION_FOR_FR': {'Hit': 0.95, 'FA': 0.95, 'Miss': 0.95},\n",
    "    'AFTERTRIAL_FR_START': {'Hit': 1, 'FA': 1, 'Miss': 1.3},  # Shock artifact is ~0.3s long\n",
    "    'AFTERTRIAL_FR_END': {'Hit': 1.9, 'FA': 1.9, 'Miss': 2.25},  # 0.95 duration to keep window consistent with trial\n",
    "    \n",
    "    'SHOCK_ARTIFACT': [0.95, 1.3],\n",
    "    # These are for raw data extraction (no FR calculations)\n",
    "    'PRETRIAL_DURATION_FOR_SPIKETIMES': 2,  # in seconds; for grabbing spiketimes around AM trials\n",
    "    'POSTTRIAL_DURATION_FOR_SPIKETIMES': 5,  # in seconds; for grabbing spiketimes around AM trials\n",
    "    \n",
    "    'DEBUG_RUN': False,  # Turns off multiprocessing for debugging\n",
    "    # For multiprocessing. Defaults to 4/5s of the number of cores\n",
    "    'NUMBER_OF_CORES': 4 * cpu_count() // 5,\n",
    "\n",
    "    # Only run these cells/subjects/sessions or None to run all\n",
    "    'SESSIONS_TO_RUN': None,  # You can specify parts of the file name too\n",
    "    'SESSIONS_TO_EXCLUDE': None,\n",
    "    \n",
    "    'OVERWRITE_PREVIOUS_CSV': True,  # False: appends to existing firing rate CSV file\n",
    "    \n",
    "    # Set up your recording platforms here\n",
    "    'RECORDING_TYPE_DICT': {\n",
    "        'SUBJ-ID-197': 'synapse',\n",
    "        'SUBJ-ID-151': 'synapse',\n",
    "        'SUBJ-ID-154': 'synapse',\n",
    "        'SUBJ-ID-231': 'intan',\n",
    "        # 'SUBJ-ID-232': 'intan',\n",
    "        # 'SUBJ-ID-270': 'intan',\n",
    "        'SUBJ-ID-389': 'intan',\n",
    "        'SUBJ-ID-390': 'intan'\n",
    "    },\n",
    "\n",
    "    # Only needed for some older recordings processed by the MatLab pipeline. Newer files contain the sampling rates in them\n",
    "    'SAMPLING_RATE_DICT': {\n",
    "        'synapse': 24414.0625,\n",
    "        'intan': 30000\n",
    "    },\n",
    "    \n",
    "    # PSTH generation settings\n",
    "    'PSTH_BIN_SIZE': 10,\n",
    "    'PSTH_PRE_STIMULUS_DURATION': 2,\n",
    "    'PSTH_POST_STIMULUS_DURATION': 4,\n",
    "    'PSTH_FIXED_YLIM': 60,\n",
    "    'PSTH_RASTER_YLIM': 30.5,\n",
    "    'PSTH_ALIGN_TO_RESPONSE': False,\n",
    "    'PSTH_TRIALTYPES': [\n",
    "        'Hit (shock)',  # Trials above threshold\n",
    "        # 'Hit (no shock)', # Trials below threshold\n",
    "        'False alarm',\n",
    "        'Miss (shock)',  # Trials above threshold\n",
    "        # 'Miss (no shock)', # Trials below threshold\n",
    "        'Passive',\n",
    "    ],\n",
    "    \n",
    "    # AuROC heatmap generation settings\n",
    "    'AUROC_GROUPING_FILE': DATA_PATH + sep + 'allUnits_list.csv',\n",
    "    'AUROC_GROUPING_VARIABLE': 'ActiveBaseline_modulation_direction',\n",
    "    'AUROC_UNIQUE_GROUPS': ['decrease', 'increase', 'none'],\n",
    "    'AUROC_GROUP_COLORS': ['#E49E50', '#5AB4E5', '#939598'],\n",
    "    'AUROC_TRIALTYPES': {\n",
    "        # Use these if you would like to sort based on trial (AM)-aligned auROC\n",
    "        'PassivePre': [0, 1.5],\n",
    "        'Hit_shockOn_auroc': [0, 1.5],\n",
    "        # 'Hit_shockOff_auroc': [0, 1.5],\n",
    "        'FA_auroc': [0, 1.5],\n",
    "        'Miss_shockOn_auroc': [1.4, 2.9],\n",
    "        # 'Miss_shockOff_auroc': [1.4, 2.9]\n",
    "        'PassivePost': [0, 1.5],\n",
    "        \n",
    "        # Use these if you would like to sort based on response (spout offset)-aligned auROC\n",
    "        # 'SpoutOff_hits_auroc': [-0.5, 0],\n",
    "        # 'SpoutOff_FAs_auroc': [-0.5, 0],\n",
    "        # 'SpoutOff_misses_auroc': [-0.5, 0]\n",
    "    },\n",
    "    \n",
    "    'SORT_BY_WHICH_TRIALTYPE': 'Hit_shockOn_auroc',\n",
    "    \n",
    "    'AUROC_BIN_SIZE': 0.1,\n",
    "    'AUROC_PRE_STIMULUS_DURATION': 2,\n",
    "    \n",
    "    'AUROC_POST_STIMULUS_DURATION': 4,\n",
    "    \n",
    "    # Below is a switchboard of functions you desire to run from the pipeline\n",
    "    # If you change your mind later, you can just run the ones you want and the code will add it to existing JSON files\n",
    "    'PIPELINE_SWITCHBOARD': {\n",
    "        # Trial-by-trial firing rates\n",
    "        'firing_rate_to_trials': True,  \n",
    "        \n",
    "        # auROC analyses need by definition to combine trials, so each of these pool a specific type of trial and align to specific events\n",
    "        # Trials aligned by AM onset\n",
    "        'auROC_hit': True,\n",
    "        'auROC_FA': True,\n",
    "        'auROC_miss': True,\n",
    "        'auROC_hitByShock': True,  # Separate trials by whether a shock was going to be delivered\n",
    "        'auROC_missByShock': True, # Separate trials by whether a shock was delivered\n",
    "        'auROC_AMTrial': True,  # Agnostic of whether miss or hit\n",
    "        'auROC_AMdepthByAMdepth': False, # Agnostic of whether miss or hit, but separated by AM depth\n",
    "        \n",
    "        # Trials aligned by spoutOff events\n",
    "        'auROC_spoutOffHit': True,\n",
    "        'auROC_spoutOffFA': True,\n",
    "        'auROC_spoutOffMiss': True,\n",
    "        'auROC_spoutOffHitByShock': True,  # Separate trials by whether a shock was going to be delivered\n",
    "        'auROC_spoutOffMissByShock': True, # Separate trials by whether a shock was delivered\n",
    "        \n",
    "        # PSTH plotting\n",
    "        'plot_trialType_PSTH': True,\n",
    "        'plot_AMDepth_PSTH': True\n",
    "    }\n",
    "}\n"
   ],
   "id": "5f703569dd7af350",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initial file matching then run the pipeline\n",
    "Uses multiprocessing to process many units at once"
   ],
   "id": "731e823a8fb4dcf8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "makedirs(SETTINGS_DICT['OUTPUT_PATH'] + sep + 'JSON files', exist_ok=True)\n",
    "\n",
    "# Load existing JSONs; will be empty if this is the first time running\n",
    "json_filenames = glob(SETTINGS_DICT['OUTPUT_PATH'] + sep + 'JSON files' + sep + '*json')\n",
    "\n",
    "# Clear older temp files if they exist\n",
    "process_tempfiles = glob(SETTINGS_DICT['OUTPUT_PATH'] + sep + '*_tempfile_*.csv')\n",
    "[remove(f) for f in process_tempfiles]\n",
    "\n",
    "# Generate a list of inputs to be passed to each worker\n",
    "input_lists = list()\n",
    "memory_paths = glob(SETTINGS_DICT['SPIKES_PATH'] + sep + '*cluster*.txt')\n",
    "\n",
    "for dummy_idx, memory_path in enumerate(memory_paths):\n",
    "    if SETTINGS_DICT['SESSIONS_TO_RUN'] is not None:\n",
    "        if any([chosen for chosen in SETTINGS_DICT['SESSIONS_TO_RUN'] if chosen in memory_path]):\n",
    "            pass\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    if SETTINGS_DICT['SESSIONS_TO_EXCLUDE'] is not None:\n",
    "        if any([chosen for chosen in SETTINGS_DICT['SESSIONS_TO_EXCLUDE'] if chosen in memory_path]):\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    if SETTINGS_DICT['DEBUG_RUN']:\n",
    "        run_pipeline((memory_path, json_filenames, SETTINGS_DICT))\n",
    "    else:\n",
    "        input_lists.append((memory_path, json_filenames, SETTINGS_DICT))\n",
    "\n",
    "if not SETTINGS_DICT['DEBUG_RUN']:\n",
    "    pool = Pool(SETTINGS_DICT['NUMBER_OF_CORES'])\n",
    "\n",
    "    # # Feed each worker with all memory paths from one unit\n",
    "    pool_map_result = pool.map(run_pipeline, input_lists)\n",
    "\n",
    "    pool.close()\n",
    "\n",
    "    pool.join()\n",
    "    \n",
    "    compile_fr_result_csv(SETTINGS_DICT['EXPERIMENT_TAG'], SETTINGS_DICT['OUTPUT_PATH'], SETTINGS_DICT['OVERWRITE_PREVIOUS_CSV'])"
   ],
   "id": "7bed86f7c4752178",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Now we can use the JSONs exported from the code above for a faster exploration of the data",
   "id": "deea365829bb96c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Sessions you want to run; edit the specifics in helpers.get_JSON_data if you want to change\n",
    "session_tags = ['pre', 'active', 'post', 'post1h']\n",
    "\n",
    "# Load existing JSONs\n",
    "json_filenames = glob(SETTINGS_DICT['OUTPUT_PATH'] + sep + 'JSON files' + sep + '*json')\n",
    "\n",
    "# Retrieve data in JSON\n",
    "data_dict = dict()\n",
    "print(\"Loading data in JSONs...\")\n",
    "for session_type in session_tags:\n",
    "    unit_list, data_list = get_JSON_data(json_filenames, session_type, \n",
    "                                                  sessions_to_run=SETTINGS_DICT['SESSIONS_TO_RUN'], \n",
    "                                                  sessions_to_exclude=SETTINGS_DICT['SESSIONS_TO_EXCLUDE'])\n",
    "    for unit_idx, unit in enumerate(unit_list):\n",
    "        if unit not in data_dict.keys():\n",
    "            data_dict.update({unit: {}})\n",
    "        data_dict[unit].update({session_type: data_list[unit_idx]})"
   ],
   "id": "5fc8863493437641",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PSTH plotting\n",
    "Unsophisticated PSTH plotting engine. Need to make tweaks if you want something fancier\n",
    "Uses multiprocessing to process many units at once"
   ],
   "id": "3f0dec45a485d216"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not SETTINGS_DICT['DEBUG_RUN']:\n",
    "    pool = Pool(SETTINGS_DICT['NUMBER_OF_CORES'])\n",
    "\n",
    "    # Feed each worker one unit_name\n",
    "    input_list = [(unit_name, data_dict[unit_name], SETTINGS_DICT) for unit_name in data_dict.keys()]\n",
    "    pool_map_result = pool.map(run_PSTH_pipeline, input_list)\n",
    "\n",
    "    pool.close()\n",
    "\n",
    "    pool.join()\n",
    "else:\n",
    "    [run_PSTH_pipeline((unit_name, data_dict[unit_name], SETTINGS_DICT)) for unit_name in data_dict.keys()]"
   ],
   "id": "8042d360245ce834",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AuROC heatmaps\n",
    "Quick summary of what auROC profiles look like across the entire population."
   ],
   "id": "3801fee70507f18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "run_auROC_heatmap_pipeline(data_dict, SETTINGS_DICT)",
   "id": "6d717d51aa97e18c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
